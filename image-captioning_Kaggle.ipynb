{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1111676,"sourceType":"datasetVersion","datasetId":623289},{"sourceId":1111749,"sourceType":"datasetVersion","datasetId":623329},{"sourceId":10740841,"sourceType":"datasetVersion","datasetId":6660447}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport einops\n\n\n#HyperParameters :\n#seting hyperparameters:\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\npatch_size = 16\nlatent_size = 512\nn_channels = 3\nnum_heads = 12\nnum_layers = 12\ndropout = 0.1\nnum_classes = 10\nsize = 224\n\nepoch = 30\nbase_lr = 10e-3\nweight_decay = 0.03\nbatch_size = 16\n\n\n\n\n\n#Multi Head Attention Mechanism:\n\nclass Head_normal(nn.Module):\n    # One head of self-attention (without masking)\n    \n    def __init__(self,latent_size = latent_size,dropout = dropout,num_heads = num_heads ):\n        super().__init__()\n\n        head_size = latent_size // num_heads\n        self.key_normal = nn.Linear(latent_size, head_size, bias=False)\n        self.query_normal = nn.Linear(latent_size, head_size, bias=False)\n        self.value_normal = nn.Linear(latent_size, head_size, bias=False)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, xk,xq,xv):\n\n        # print('     Entered Head Normal\\n')\n        \n        k = self.key_normal(xk)  # (B, T, head_size)\n        q = self.query_normal(xq)   # (B, T, head_size)\n\n        # Compute attention scores (affinities)\n        wei = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5  # (B, T, T)\n        wei = F.softmax(wei, dim=-1)  # No masking applied\n        wei = self.dropout(wei)\n        # print(wei.shape)\n        # Perform weighted aggregation of values\n        v = self.value_normal(xv)  # (B, T, head_size)\n        # print(v.shape)\n        out = wei @ v  # (B, T, head_size)\n        # print('     Exited Head Normal\\n')\n        return out \n\nclass MultiHeadAttention(nn.Module):\n    # Multiple heads of self-attention in parallel (Unmasked)\n    \n    def __init__(self,latent_size = latent_size, num_head = num_heads, dropout = dropout):\n        super().__init__()\n        head_size = latent_size // num_head\n        self.heads = nn.ModuleList([Head_normal() for _ in range(num_head)])  # Multiple heads\n        self.proj = nn.Linear(head_size * num_head, latent_size)  # Projection layer\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, xk,xq,xv):\n        # print(' Entered MultiHead\\n')\n        out = torch.cat([h(xk,xq,xv) for h in self.heads], dim=-1)  # Concatenate outputs from all heads\n        out = self.dropout(self.proj(out))  # Apply projection and dropout\n        # print(' Exited Multihead\\n')\n        return out\n    \n\n#Input Embedding for Vision Transformer:\n# 1. Create a class which subclasses nn.Module\nclass InputEmbedding(nn.Module):\n    \"\"\"Turns a 2D input image into a 1D sequence learnable embedding vector.\n\n    Args:\n        in_channels (int): Number of color channels for the input images. Defaults to 3.\n        patch_size (int): Size of patches to convert input image into. Defaults to 16.\n        embedding_dim (int): Size of embedding to turn image into. Defaults to 768.\n    \"\"\"\n    # 2. Initialize the class with appropriate variables\n    def __init__(self,patch_size =patch_size,n_channels =n_channels,device= device,latent_size = latent_size,batch_size= batch_size):\n        super().__init__()\n\n\n        self.patch_size = patch_size\n        self.latent_size =latent_size\n        self.n_channels = n_channels\n        self.device = device\n        self.batch_size = batch_size\n        self.input_size = self.patch_size *self.patch_size * self.n_channels\n\n        # 3. Create a layer to turn an image into patches\n        self.patcher = nn.Conv2d(in_channels=3,\n                                 out_channels=self.latent_size,\n                                 kernel_size=16,\n                                 stride=16,\n                                 padding=0)\n\n        # 4. Create a layer to flatten the patch feature maps into a single dimension\n        self.flatten = nn.Flatten(start_dim=2, # only flatten the feature map dimensions into a single vector\n                                  end_dim=3)\n        \n\n        # class token\n        self.class_token = nn.Parameter(torch.randn(self.batch_size,1,self.latent_size)).to(self.device)\n        # print(self.class_token.shape)\n        # position embedding\n        self.pos_embedding = nn.Parameter(torch.randn(self.batch_size,1,self.latent_size)).to(self.device)\n\n    # 5. Define the forward method\n    def forward(self, x):\n        # Create assertion to check that inputs are the correct shape\n        image_resolution = x.shape[-1]\n        assert image_resolution % patch_size == 0, f\"Input image size must be divisible by patch size, image shape: {image_resolution}, patch size: {patch_size}\"\n\n        # Perform the forward pass\n        x_patched = self.patcher(x)\n        # print(\"X_patches size:\",x_patched.shape)\n        x_flattened = self.flatten(x_patched)\n\n        # print(\"x_flatted normal:\", x_flattened.shape)\n\n        x_flattened = x_flattened.permute(0, 2, 1).to(self.device)\n        b , n, _ = x_flattened.shape\n\n        # print(\"x_flattened permuted:\", x_flattened.shape)\n\n        liner_projection = torch.cat((self.class_token, x_flattened), dim=1)\n        # print(liner_projection.shape)\n        pos_embedding = einops.repeat(self.pos_embedding, 'b 1 d -> b m d', m = n+1)\n\n        liner_projection = liner_projection +pos_embedding\n        \n        return liner_projection# adjust so the embedding is on the final dimension [batch_size, P^2•C, N] -> [batch_size, N, P^2•C]\n    \n\n#Encoder Block For ViT:\nclass EncoderBlock(nn.Module):\n    def __init__(self,latent_size = latent_size, num_heads = num_heads, device = device, dropout = dropout):\n        super(EncoderBlock,self).__init__()\n\n        self.latent_size = latent_size\n        self.num_heads = num_heads\n        self.device = device\n        self.dropout = dropout\n\n        #nOrm layer\n\n        self.norm = nn.LayerNorm(self.latent_size)\n\n        #MULTIHEADATTENTION\n\n        # self.multihead = nn.MultiheadAttention(\n        #     self.latent_size, self.num_heads, self.dropout\n        # )\n\n        self.multihead = MultiHeadAttention()\n\n        # nn.MultiheadAttention()\n\n        self.enc_MLP = nn.Sequential(\n            nn.Linear(self.latent_size, self.latent_size *4),\n            nn.GELU(),\n            nn.Dropout(self.dropout),\n            nn.Linear(self.latent_size *4 , self.latent_size),\n            nn.Dropout(self.dropout)\n            \n        )\n\n    \n    def forward(self, embedded_patches):\n\n        first_norm = self.norm(embedded_patches)\n        attention_out = self.multihead(first_norm,first_norm,first_norm)[0]\n\n        # first_residuloa_connetion\n\n        first_added = attention_out + embedded_patches\n\n        second_norm =self.norm(first_added)\n\n        ff_output = self.enc_MLP(second_norm)\n\n\n        return ff_output + first_added\n    \n\n#puting everything toggether:\n\nclass VitModel(nn.Module):\n    def __init__(self,num_encoders = num_layers, latent_size = latent_size, device = device,num_classes = num_classes, dropout = dropout):\n        super(VitModel,self).__init__()\n\n        self.num_encoders = num_encoders\n        self.latent_size =latent_size\n        self.device = device\n        self.dropout = dropout\n        self.num_classes = num_classes\n\n        self.embd = InputEmbedding()\n\n        self.encstack = nn.ModuleList([ EncoderBlock() for i in range(self.num_encoders)])\n\n\n    def forward(self, test_input):\n\n        enc_output = self.embd(test_input)\n\n        for enc_layer in self.encstack:\n            enc_output = enc_layer.forward(enc_output)\n        \n        \n\n        return enc_output\n        \n\n    \n\n# model = VitModel().to(device)\n# test_input = torch.randn((16,3,224,224)).to(device)\n# print(model(test_input).shape)\n\n\n# print(sum(p.numel() for p in model.parameters() if p.requires_grad))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-13T13:47:47.015856Z","iopub.execute_input":"2025-02-13T13:47:47.016172Z","iopub.status.idle":"2025-02-13T13:47:47.052189Z","shell.execute_reply.started":"2025-02-13T13:47:47.016150Z","shell.execute_reply":"2025-02-13T13:47:47.051371Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport math\n\nclass LayerNormalization(nn.Module):\n\n    def __init__(self, features: int, eps:float=10**-6) -> None:\n        super().__init__()\n        self.eps = eps\n        self.alpha = nn.Parameter(torch.ones(features)) # alpha is a learnable parameter\n        self.bias = nn.Parameter(torch.zeros(features)) # bias is a learnable parameter\n\n    def forward(self, x):\n        # x: (batch, seq_len, hidden_size)\n         # Keep the dimension for broadcasting\n        mean = x.mean(dim = -1, keepdim = True) # (batch, seq_len, 1)\n        # Keep the dimension for broadcasting\n        std = x.std(dim = -1, keepdim = True) # (batch, seq_len, 1)\n        # eps is to prevent dividing by zero or when std is very small\n        return self.alpha * (x - mean) / (std + self.eps) + self.bias\n\nclass FeedForwardBlock(nn.Module):\n\n    def __init__(self, d_model: int, d_ff: int, dropout: float) -> None:\n        super().__init__()\n        self.linear_1 = nn.Linear(d_model, d_ff) # w1 and b1\n        self.dropout = nn.Dropout(dropout)\n        self.linear_2 = nn.Linear(d_ff, d_model) # w2 and b2\n\n    def forward(self, x):\n        # (batch, seq_len, d_model) --> (batch, seq_len, d_ff) --> (batch, seq_len, d_model)\n        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))\n\nclass InputEmbeddings(nn.Module):\n\n    def __init__(self, d_model: int, vocab_size: int) -> None:\n        super().__init__()\n        self.d_model = d_model\n        self.vocab_size = vocab_size\n        self.embedding = nn.Embedding(vocab_size, d_model)\n\n    def forward(self, x):\n        # (batch, seq_len) --> (batch, seq_len, d_model)\n        # Multiply by sqrt(d_model) to scale the embeddings according to the paper\n        return self.embedding(x.long()) * math.sqrt(self.d_model)\n\n    \nclass PositionalEncoding(nn.Module):\n\n    def __init__(self, d_model: int, seq_len: int, dropout: float) -> None:\n        super().__init__()\n        self.d_model = d_model\n        self.seq_len = seq_len\n        self.dropout = nn.Dropout(dropout)\n        # Create a matrix of shape (seq_len, d_model)\n        pe = torch.zeros(seq_len, d_model)\n        # Create a vector of shape (seq_len)\n        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1) # (seq_len, 1)\n        # Create a vector of shape (d_model)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) # (d_model / 2)\n        # Apply sine to even indices\n        pe[:, 0::2] = torch.sin(position * div_term) # sin(position * (10000 ** (2i / d_model))\n        # Apply cosine to odd indices\n        pe[:, 1::2] = torch.cos(position * div_term) # cos(position * (10000 ** (2i / d_model))\n        # Add a batch dimension to the positional encoding\n        pe = pe.unsqueeze(0) # (1, seq_len, d_model)\n        # Register the positional encoding as a buffer\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False) # (batch, seq_len, d_model)\n        return self.dropout(x)\n\nclass ResidualConnection(nn.Module):\n    \n        def __init__(self, features: int, dropout: float) -> None:\n            super().__init__()\n            self.dropout = nn.Dropout(dropout)\n            self.norm = LayerNormalization(features)\n    \n        def forward(self, x, sublayer):\n            return x + self.dropout(sublayer(self.norm(x)))\n\nclass MultiHeadAttentionBlock(nn.Module):\n\n    def __init__(self, d_model: int, h: int, dropout: float) -> None:\n        super().__init__()\n        self.d_model = d_model # Embedding vector size\n        self.h = h # Number of heads\n        # Make sure d_model is divisible by h\n        assert d_model % h == 0, \"d_model is not divisible by h\"\n\n        self.d_k = d_model // h # Dimension of vector seen by each head\n        self.w_q = nn.Linear(d_model, d_model, bias=False) # Wq\n        self.w_k = nn.Linear(d_model, d_model, bias=False) # Wk\n        self.w_v = nn.Linear(d_model, d_model, bias=False) # Wv\n        self.w_o = nn.Linear(d_model, d_model, bias=False) # Wo\n        self.dropout = nn.Dropout(dropout)\n\n    @staticmethod\n    def attention(query, key, value, mask, dropout: nn.Dropout):\n        d_k = query.shape[-1]\n        # Just apply the formula from the paper\n        # (batch, h, seq_len, d_k) --> (batch, h, seq_len, seq_len)\n        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n        if mask is not None:\n            # Write a very low value (indicating -inf) to the positions where mask == 0\n            attention_scores.masked_fill_(mask == 0, -1e4)\n        attention_scores = attention_scores.softmax(dim=-1) # (batch, h, seq_len, seq_len) # Apply softmax\n        if dropout is not None:\n            attention_scores = dropout(attention_scores)\n        # (batch, h, seq_len, seq_len) --> (batch, h, seq_len, d_k)\n        # return attention scores which can be used for visualization\n        return (attention_scores @ value), attention_scores\n\n    def forward(self, q, k, v, mask):\n        query = self.w_q(q) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n        key = self.w_k(k) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n        value = self.w_v(v) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n\n        # (batch, seq_len, d_model) --> (batch, seq_len, h, d_k) --> (batch, h, seq_len, d_k)\n        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1, 2)\n        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1, 2)\n        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1, 2)\n\n        # Calculate attention\n        x, self.attention_scores = MultiHeadAttentionBlock.attention(query, key, value, mask, self.dropout)\n        \n        # Combine all the heads together\n        # (batch, h, seq_len, d_k) --> (batch, seq_len, h, d_k) --> (batch, seq_len, d_model)\n        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.h * self.d_k)\n\n        # Multiply by Wo\n        # (batch, seq_len, d_model) --> (batch, seq_len, d_model)  \n        return self.w_o(x)\n\nclass EncoderBlock_1(nn.Module):\n\n    def __init__(self, features: int, self_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n        super().__init__()\n        self.self_attention_block = self_attention_block\n        self.feed_forward_block = feed_forward_block\n        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(2)])\n\n    def forward(self, x, src_mask):\n        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, src_mask))\n        x = self.residual_connections[1](x, self.feed_forward_block)\n        return x\n    \nclass Encoder(nn.Module):\n\n    def __init__(self, features: int, layers: nn.ModuleList) -> None:\n        super().__init__()\n        self.layers = layers\n        self.norm = LayerNormalization(features)\n\n    def forward(self, x, mask):\n        for layer in self.layers:\n            x = layer(x, mask)\n        return self.norm(x)\n\nclass DecoderBlock_1(nn.Module):\n\n    def __init__(self, features: int, self_attention_block: MultiHeadAttentionBlock, cross_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n        super().__init__()\n        self.self_attention_block = self_attention_block\n        self.cross_attention_block = cross_attention_block\n        self.feed_forward_block = feed_forward_block\n        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(3)])\n\n    def forward(self, x, encoder_output, src_mask, tgt_mask):\n        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, tgt_mask))\n        x = self.residual_connections[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, src_mask))\n        x = self.residual_connections[2](x, self.feed_forward_block)\n        return x\n    \nclass Decoder(nn.Module):\n\n    def __init__(self, features: int, layers: nn.ModuleList) -> None:\n        super().__init__()\n        self.layers = layers\n        self.norm = LayerNormalization(features)\n\n    def forward(self, x, encoder_output, src_mask, tgt_mask):\n        for layer in self.layers:\n            x = layer(x, encoder_output, src_mask, tgt_mask)\n        return self.norm(x)\n\nclass ProjectionLayer(nn.Module):\n\n    def __init__(self, d_model, vocab_size) -> None:\n        super().__init__()\n        self.proj = nn.Linear(d_model, vocab_size)\n\n    def forward(self, x) -> None:\n        # (batch, seq_len, d_model) --> (batch, seq_len, vocab_size)\n        return self.proj(x)\n    \nclass Transformer(nn.Module):\n\n    def __init__(self, encoder: Encoder, decoder: Decoder, tgt_embed: InputEmbeddings, tgt_pos: PositionalEncoding, projection_layer: ProjectionLayer) -> None:\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.tgt_embed = tgt_embed\n        self.tgt_pos = tgt_pos\n        self.projection_layer = projection_layer\n\n    def encode(self, src, src_mask):\n        # (batch, seq_len, d_model)\n        return self.encoder(src, src_mask)\n    \n    def decode(self, encoder_output: torch.Tensor, src_mask: torch.Tensor, tgt: torch.Tensor, tgt_mask: torch.Tensor):\n        # (batch, seq_len, d_model)\n        tgt = self.tgt_embed(tgt)\n        tgt = self.tgt_pos(tgt)\n        return self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n    \n    def project(self, x):\n        # (batch, seq_len, vocab_size)\n        return self.projection_layer(x)\n    \ndef build_transformer(tgt_vocab_size: int, tgt_seq_len: int, d_model: int=512, N: int=12, h: int=8, dropout: float=0.1, d_ff: int=2048) -> Transformer:\n    # Create the embedding layers\n    \n    tgt_embed = InputEmbeddings(d_model, tgt_vocab_size)\n\n    # Create the positional encoding layers\n    \n    tgt_pos = PositionalEncoding(d_model, tgt_seq_len, dropout)\n    \n    # Create the encoder blocks\n    encoder_blocks = []\n    for _ in range(N):\n        encoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n        encoder_block = EncoderBlock_1(d_model, encoder_self_attention_block, feed_forward_block, dropout)\n        encoder_blocks.append(encoder_block)\n\n    # Create the decoder blocks\n    decoder_blocks = []\n    for _ in range(N):\n        decoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n        decoder_cross_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n        decoder_block = DecoderBlock_1(d_model, decoder_self_attention_block, decoder_cross_attention_block, feed_forward_block, dropout)\n        decoder_blocks.append(decoder_block)\n    \n    # Create the encoder and decoder\n    encoder = Encoder(d_model, nn.ModuleList(encoder_blocks))\n    decoder = Decoder(d_model, nn.ModuleList(decoder_blocks))\n    \n    # Create the projection layer\n    projection_layer = ProjectionLayer(d_model, tgt_vocab_size)\n    \n    # Create the transformer\n    transformer = Transformer(encoder, decoder, tgt_embed, tgt_pos, projection_layer)\n    \n    # Initialize the parameters\n    for p in transformer.parameters():\n        if p.dim() > 1:\n            nn.init.xavier_uniform_(p)\n    \n    return transformer\n\n\n# model = build_transformer(15698,197,768,12,8,0.1,9216)\n\n# print(sum(p.numel() for p in model.parameters() if p.requires_grad))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T13:47:47.161954Z","iopub.execute_input":"2025-02-13T13:47:47.162185Z","iopub.status.idle":"2025-02-13T13:47:47.189724Z","shell.execute_reply.started":"2025-02-13T13:47:47.162166Z","shell.execute_reply":"2025-02-13T13:47:47.188990Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n# from Vit_Model import VitModel\n# from transformer_model import build_transformer\nfrom PIL import Image\nimport torchvision.transforms as transforms\n# from data_loader import causal_mask\n\n\n\nbatch_size = 16\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\ndevice = torch.device(device)\n# print(device)\n\ndef print_examples(model, device, dataset):\n    transform = transforms.Compose(\n        [\n            transforms.Resize((224, 224)),\n            transforms.ToTensor(),\n            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n        ]\n    )\n\n    model.eval()\n    test_img1 = transform(Image.open(\"/kaggle/input/example/dog.jpeg\").convert(\"RGB\")).unsqueeze(\n        0\n    )\n    print(\"Example 1 CORRECT: Dog on a beach by the ocean\")\n    print(\n        \"Example 1 OUTPUT: \"\n        + \" \".join(model.Caption_Generation(test_img1.to(device), dataset.vocab))\n    )\n    test_img2 = transform(\n        Image.open(\"/kaggle/input/example/child_frisbee.jpeg\").convert(\"RGB\")\n    ).unsqueeze(0)\n    print(\"Example 2 CORRECT: Child holding red frisbee outdoors\")\n    print(\n        \"Example 2 OUTPUT: \"\n        + \" \".join(model.Caption_Generation(test_img2.to(device), dataset.vocab))\n    )\n    test_img3 = transform(Image.open(\"/kaggle/input/example/bus_car.jpeg\").convert(\"RGB\")).unsqueeze(\n        0\n    )\n    print(\"Example 3 CORRECT: Bus driving by parked cars\")\n    print(\n        \"Example 3 OUTPUT: \"\n        + \" \".join(model.Caption_Generation(test_img3.to(device), dataset.vocab))\n    )\n    test_img4 = transform(\n        Image.open(\"/kaggle/input/example/small_boat.jpeg\").convert(\"RGB\")\n    ).unsqueeze(0)\n    print(\"Example 4 CORRECT: A small boat in the ocean\")\n    print(\n        \"Example 4 OUTPUT: \"\n        + \" \".join(model.Caption_Generation(test_img4.to(device), dataset.vocab))\n    )\n    test_img5 = transform(\n        Image.open(\"/kaggle/input/example/cow_boy.jpeg\").convert(\"RGB\")\n    ).unsqueeze(0)\n    print(\"Example 5 CORRECT: A cowboy riding a horse in the desert\")\n    print(\n        \"Example 5 OUTPUT: \"\n        + \" \".join(model.Caption_Generation(test_img5.to(device), dataset.vocab))\n    )\n\n\n\n\nclass VisionWithTransformer(nn.Module):\n    def __init__(self,vocab_size):\n        super(VisionWithTransformer,self).__init__()\n\n\n        self.vocab_size = vocab_size\n\n        self.vision_model = VitModel()\n\n        self.transformer_model = build_transformer(tgt_vocab_size=self.vocab_size,tgt_seq_len=100,d_model=512,N=12,h=8,dropout=0.1,d_ff=2048)\n\n        self.params = list(self.vision_model.parameters()) + list(self.transformer_model.parameters())\n\n    def forward(self,imgs,encoder_mask,decoder_input,decoder_mask):\n        \n        \n\n\n        encoder_input = self.vision_model(imgs)\n\n        encoder_output = self.transformer_model.encode(encoder_input,encoder_mask)\n        decoder_output = self.transformer_model.decode(encoder_output,encoder_mask,decoder_input,decoder_mask)\n        proj_output = self.transformer_model.project(decoder_output)\n\n\n        return proj_output\n    \n\n    def Caption_Generation(self,imgs,vocab,max_length = 50):\n\n        sos_idx = vocab.stoi[\"<SOS>\"]\n        eos_idx = vocab.stoi[\"<EOS>\"]\n\n        # print(\"Image Size:\",imgs.shape)\n\n        input = []\n        input = torch.tensor(input).to(device)\n        for _ in range(batch_size):\n            input=torch.cat((input,imgs),dim=0)\n\n        # print(\"Input Size:\",input.shape)\n        source = self.vision_model(input)\n        # print(\"Source Size:\",source.shape)\n\n        source = source[0,:,:].unsqueeze(0)\n        # print(\"Source Size:\",source.shape)\n        # print(\"Source Type:\", type(source))\n        encoder_mask = None\n        decoder_input = torch.empty(1, 1).fill_(sos_idx).type_as(source).to(device)\n        # print(\"Decoder Input Shape:\",decoder_input.shape)\n        encoder_output = self.transformer_model.encode(source,encoder_mask)\n        # print(\"Encoder Output Size:\",encoder_output.shape)\n        while True:\n            if decoder_input.size(1) == max_length:\n                break\n\n            # build mask for target\n            decoder_mask = causal_mask(decoder_input.size(1)).type_as(source).to(device)\n            # print(\"Decoder Maskk size:\", decoder_mask.shape)\n            decoder_output = self.transformer_model.decode(encoder_output,encoder_mask,decoder_input,decoder_mask)\n            # print(\"Decoder output size:\", decoder_output.shape)\n            \n\n            prob = self.transformer_model.project(decoder_output[:,-1])\n            # print(\"Final output size:\", prob.size)\n            _, next_word = torch.max(prob, dim=1)\n\n            decoder_input = torch.cat(\n            [decoder_input, torch.empty(1, 1).type_as(source).fill_(next_word.item()).to(device)], dim=1\n            )\n\n\n\n            if next_word == eos_idx:\n                break\n        # print(decoder_input.int())\n        return [vocab.itos[int(idx)] for idx in decoder_input.view(-1).tolist()]\n\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T13:47:47.190868Z","iopub.execute_input":"2025-02-13T13:47:47.191176Z","iopub.status.idle":"2025-02-13T13:47:47.207773Z","shell.execute_reply.started":"2025-02-13T13:47:47.191148Z","shell.execute_reply":"2025-02-13T13:47:47.206994Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import os  # when loading file paths\nimport pandas as pd  # for lookup in annotation file\nimport spacy  # for tokenizer\nimport torch\nfrom torch.nn.utils.rnn import pad_sequence  # pad batch\nfrom torch.utils.data import DataLoader, Dataset\nfrom PIL import Image  # Load img\nimport torchvision.transforms as transforms\n\n\n# We want to convert text -> numerical values\n# 1. We need a Vocabulary mapping each word to a index\n# 2. We need to setup a Pytorch dataset to load the data\n# 3. Setup padding of every batch (all examples should be\n#    of same seq_len and setup dataloader)\n# Note that loading the image is very easy compared to the text!\n\n# Download with: python -m spacy download en\nspacy_eng = spacy.load(\"en_core_web_sm\")\n\nseq_len = 100\n\nclass Vocabulary:\n    def __init__(self, freq_threshold):\n        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n        self.stoi = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n        self.freq_threshold = freq_threshold\n\n    def __len__(self):\n        return len(self.itos)\n\n    @staticmethod\n    def tokenizer_eng(text):\n        return [tok.text.lower() for tok in spacy_eng.tokenizer(str(text))]\n\n    def build_vocabulary(self, sentence_list):\n        frequencies = {}\n        idx = 4\n\n        for sentence in sentence_list:\n            for word in self.tokenizer_eng(sentence):\n                if word not in frequencies:\n                    frequencies[word] = 1\n\n                else:\n                    frequencies[word] += 1\n\n                if frequencies[word] == self.freq_threshold:\n                    self.stoi[word] = idx\n                    self.itos[idx] = word\n                    idx += 1\n\n    def numericalize(self, text):\n        tokenized_text = self.tokenizer_eng(text)\n\n        return [\n            self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"]\n            for token in tokenized_text\n        ]\n\ndef causal_mask(size):\n    mask = torch.triu(torch.ones((1, size, size)), diagonal=1).type(torch.int)\n    return mask == 0\n\n\nclass FlickrDataset(Dataset):\n    def __init__(self, root_dir, captions_file, transform=None, freq_threshold=5):\n        self.root_dir = root_dir\n        self.df = pd.read_csv(captions_file)\n        self.transform = transform\n\n        # Get img, caption columns\n        self.imgs = self.df[\"image\"]\n        self.captions = self.df[\"caption\"]\n\n        # Initialize vocabulary and build vocab\n        self.vocab = Vocabulary(freq_threshold)\n        self.vocab.build_vocabulary(self.captions.tolist())\n\n\n    \n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, index):\n        caption = self.captions[index]\n        img_id = self.imgs[index]\n\n        self.pad_token = self.vocab.stoi[\"<PAD>\"]\n\n\n\n        img = Image.open(os.path.join(self.root_dir, img_id)).convert(\"RGB\")\n\n        if self.transform is not None:\n            img = self.transform(img)\n\n        decoder_input = [self.vocab.stoi[\"<SOS>\"]]\n        decoder_input += self.vocab.numericalize(caption)\n        decoder_input.append(self.vocab.stoi[\"<EOS>\"])\n\n        num_pad_tokens_input = seq_len - len(decoder_input)\n\n        for _ in range(num_pad_tokens_input):\n            decoder_input.append(self.pad_token)\n\n        decoder_input = torch.tensor(decoder_input)\n\n        tgt_mask = ((decoder_input != self.pad_token).unsqueeze(0).int() & causal_mask(decoder_input.size(0))).clone().detach()\n\n        label = []\n\n        label = label + self.vocab.numericalize(caption)\n        label.append(self.vocab.stoi[\"<EOS>\"])\n\n        num_pad_tokens_label = seq_len - len(label)\n\n        for _ in range(num_pad_tokens_label):\n            label.append(self.pad_token)\n\n        label = torch.tensor(label)\n\n        return img, decoder_input, tgt_mask, label\n\ndef get_loader(\n    root_folder,\n    annotation_file,\n    transform,\n    batch_size=16,\n    num_workers=,\n    shuffle=True,\n    pin_memory=True,\n):\n    dataset = FlickrDataset(root_folder, annotation_file, transform=transform)\n\n    pad_idx = dataset.vocab.stoi[\"<PAD>\"]\n\n    loader = DataLoader(\n        dataset=dataset,\n        batch_size=batch_size,\n        num_workers=num_workers,\n        shuffle=shuffle,\n        \n        pin_memory=pin_memory,\n        \n        drop_last=True\n    )\n\n    return loader, dataset\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T13:47:47.209111Z","iopub.execute_input":"2025-02-13T13:47:47.209376Z","iopub.status.idle":"2025-02-13T13:47:51.549563Z","shell.execute_reply.started":"2025-02-13T13:47:47.209348Z","shell.execute_reply":"2025-02-13T13:47:51.548888Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import kagglehub\n\n# Download latest version\npath = kagglehub.dataset_download(\"adityajn105/flickr30k\")\n\nprint(\"Path to dataset files:\", path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T13:47:51.550389Z","iopub.execute_input":"2025-02-13T13:47:51.550759Z","iopub.status.idle":"2025-02-13T13:47:52.940377Z","shell.execute_reply.started":"2025-02-13T13:47:51.550739Z","shell.execute_reply":"2025-02-13T13:47:52.939533Z"}},"outputs":[{"name":"stdout","text":"Path to dataset files: /kaggle/input/flickr30k\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import torch \nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.transforms as transforms\n\n# from model_imagecaptioning import VisionWithTransformer,print_examples\n# from data_loader import get_loader\nfrom tqdm import tqdm\n\n# Ensure that the script runs properly on Windows\nif __name__ == \"__main__\":\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    print(\"Using Device:\", device)\n    if device == \"cuda\":\n        print(f\"Device name: {torch.cuda.get_device_name(0)}\")\n        print(f\"Device memory: {torch.cuda.get_device_properties(0).total_memory / 1024 ** 3} GB\")\n    else:\n        print(\"NOTE: If you have a GPU, consider using it for training.\")\n\n    device = torch.device(device)\n\n    transform = transforms.Compose(\n        [\n            transforms.Resize((224, 224)),\n            transforms.ToTensor(),\n            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n        ]\n    )\n\n    # DataLoader\n    loader, dataset = get_loader(\n        \"/kaggle/input/flickr8k/Images\", \"/kaggle/input/flickr8k/captions.txt\", transform=transform\n    )\n\n    vocab_size = len(dataset.vocab)\n    print(\"Total number of words in vocab:\", vocab_size)\n    # print(\"vOCAB:\", dataset.vocab)\n    model = VisionWithTransformer(vocab_size=vocab_size).to(device)\n\n    params = model.parameters()\n\n    loss_fn = nn.CrossEntropyLoss(ignore_index=dataset.vocab.stoi[\"<PAD>\"], label_smoothing=0.1).to(device)\n    optimizer = optim.AdamW(params, lr=3e-4, weight_decay=1e-4, betas=(0.9, 0.98))\n    print(\"Total number of learnable parameters:\",sum(p.numel() for p in model.parameters() if p.requires_grad))\n\n    # model_filename = \"/kaggle/working/saved_model.pt\"\n    # if model_filename:\n    #     print(f'Preloading model {model_filename}')\n    #     state = torch.load(model_filename)\n    #     model.load_state_dict(state['model_state_dict'])\n    #     # initial_epoch = state['epoch'] + 1\n    #     optimizer.load_state_dict(state['optimizer_state_dict'])\n    #     # global_step = state['global_step']\n    # else:\n    #     print('No model to preload, starting from scratch')\n\n    \n\n    # # scaler = torch.amp.GradScaler('cuda')\n\n    for epoch in range(30):\n        print_examples(model,device,dataset)\n        torch.cuda.empty_cache()\n        model.train()\n        batch_iterator = tqdm(loader, desc=f\"Processing Epoch {epoch:02d}\")\n        for imgs, captions, tgt_masks, labels in batch_iterator:\n            # print_examples(model,device,dataset)\n            # break\n            \n            optimizer.zero_grad(set_to_none=True)\n            imgs = imgs.to(device)\n            \n            captions = captions.to(device)\n            \n            tgt_masks = tgt_masks.to(device)\n            \n            labels = labels.to(device)\n            \n\n            \n            \n\n            \n                \n            output = model(imgs,None,captions,tgt_masks)\n                \n            loss = loss_fn(output.view(-1, vocab_size), labels.view(-1))\n            \n            batch_iterator.set_postfix({\"loss\": f\"{loss.item():6.3f}\"})\n\n            \n            loss.backward()\n            \n            optimizer.step()\n\n\n            \n\n        \n\n        model_filename = \"/kaggle/working/saved_model.pt\" \n        torch.save({\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            # 'global_step': global_step\n        }, model_filename)\n        print(\"Model Saved successfully\")\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T13:47:52.941280Z","iopub.execute_input":"2025-02-13T13:47:52.941730Z","execution_failed":"2025-02-14T01:47:21.537Z"}},"outputs":[{"name":"stdout","text":"Using Device: cuda\nDevice name: Tesla P100-PCIE-16GB\nDevice memory: 15.887939453125 GB\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Total number of words in vocab: 2994\nTotal number of learnable parameters: 129269170\nExample 1 CORRECT: Dog on a beach by the ocean\nExample 1 OUTPUT: <SOS> laid hugs laid laid laid laid laid robot robot lot robot lot robot watery laid laid laid laid laid laid laid laid laid laid laid laid watery watery watery watery watery watery watery watery watery watery watery watery watery watery watery watery watery watery watery watery watery watery watery\nExample 2 CORRECT: Child holding red frisbee outdoors\nExample 2 OUTPUT: <SOS> laid laid laid laid laid laid laid laid whilst laid whilst laid laid laid laid laid laid laid laid laid laid laid laid laid laid laid laid laid watery watery watery watery watery watery watery watery watery watery telephone telephone watery watery watery watery laid watery telephone laid watery\nExample 3 CORRECT: Bus driving by parked cars\nExample 3 OUTPUT: <SOS> laid laid laid laid laid laid laid whilst formal whilst formal laid laid laid laid laid laid laid laid laid laid laid laid laid laid laid laid watery watery watery watery watery watery watery watery watery watery watery watery watery watery watery watery watery watery watery watery watery watery\nExample 4 CORRECT: A small boat in the ocean\nExample 4 OUTPUT: <SOS> laid whilst laid laid laid laid laid whilst laid whilst whilst watery telephone telephone telephone telephone laid laid laid laid laid laid laid laid laid laid laid watery watery watery watery watery watery watery watery watery watery watery watery watery watery watery watery watery watery watery watery watery watery\nExample 5 CORRECT: A cowboy riding a horse in the desert\nExample 5 OUTPUT: <SOS> laid whilst laid laid laid laid laid laid whilst whilst whilst laid laid laid whilst laid laid laid laid laid laid laid laid laid laid laid watery watery watery watery watery watery watery watery watery watery watery watery watery watery watery watery watery watery watery watery watery watery watery\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 00: 100%|██████████| 2528/2528 [24:03<00:00,  1.75it/s, loss=4.069]\n","output_type":"stream"},{"name":"stdout","text":"Example 1 CORRECT: Dog on a beach by the ocean\nExample 1 OUTPUT: <SOS> a man in a red shirt is climbing a rock . <EOS>\nExample 2 CORRECT: Child holding red frisbee outdoors\nExample 2 OUTPUT: <SOS> a man in a red shirt is climbing a rock . <EOS>\nExample 3 CORRECT: Bus driving by parked cars\nExample 3 OUTPUT: <SOS> a man in a red shirt is standing on a sidewalk . <EOS>\nExample 4 CORRECT: A small boat in the ocean\nExample 4 OUTPUT: <SOS> a man in a red shirt is climbing a rock . <EOS>\nExample 5 CORRECT: A cowboy riding a horse in the desert\nExample 5 OUTPUT: <SOS> a man in a red shirt is climbing a rock . <EOS>\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 01: 100%|██████████| 2528/2528 [24:03<00:00,  1.75it/s, loss=3.750]\n","output_type":"stream"},{"name":"stdout","text":"Example 1 CORRECT: Dog on a beach by the ocean\nExample 1 OUTPUT: <SOS> a man in a blue shirt is standing in front of a large building . <EOS>\nExample 2 CORRECT: Child holding red frisbee outdoors\nExample 2 OUTPUT: <SOS> a man in a white shirt and white shirt is standing in front of a large building . <EOS>\nExample 3 CORRECT: Bus driving by parked cars\nExample 3 OUTPUT: <SOS> a man in a white shirt is standing in front of a large building . <EOS>\nExample 4 CORRECT: A small boat in the ocean\nExample 4 OUTPUT: <SOS> a man in a white jacket is standing in front of a large building . <EOS>\nExample 5 CORRECT: A cowboy riding a horse in the desert\nExample 5 OUTPUT: <SOS> a man in a blue shirt is standing in front of a large building . <EOS>\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 02: 100%|██████████| 2528/2528 [24:04<00:00,  1.75it/s, loss=3.298]\n","output_type":"stream"},{"name":"stdout","text":"Example 1 CORRECT: Dog on a beach by the ocean\nExample 1 OUTPUT: <SOS> a dog is running through the water . <EOS>\nExample 2 CORRECT: Child holding red frisbee outdoors\nExample 2 OUTPUT: <SOS> a brown dog is running through a field . <EOS>\nExample 3 CORRECT: Bus driving by parked cars\nExample 3 OUTPUT: <SOS> a man in a blue shirt is standing on a rock . <EOS>\nExample 4 CORRECT: A small boat in the ocean\nExample 4 OUTPUT: <SOS> a dog is running through the snow . <EOS>\nExample 5 CORRECT: A cowboy riding a horse in the desert\nExample 5 OUTPUT: <SOS> a dog is running through the water . <EOS>\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 03: 100%|██████████| 2528/2528 [24:06<00:00,  1.75it/s, loss=3.555]\n","output_type":"stream"},{"name":"stdout","text":"Example 1 CORRECT: Dog on a beach by the ocean\nExample 1 OUTPUT: <SOS> a young boy is swimming in a pool . <EOS>\nExample 2 CORRECT: Child holding red frisbee outdoors\nExample 2 OUTPUT: <SOS> a dog is running through a field . <EOS>\nExample 3 CORRECT: Bus driving by parked cars\nExample 3 OUTPUT: <SOS> a man in a blue shirt is standing in front of a mountain . <EOS>\nExample 4 CORRECT: A small boat in the ocean\nExample 4 OUTPUT: <SOS> a man is standing in the ocean . <EOS>\nExample 5 CORRECT: A cowboy riding a horse in the desert\nExample 5 OUTPUT: <SOS> a man is standing on a rock in front of a mountain . <EOS>\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 04: 100%|██████████| 2528/2528 [24:07<00:00,  1.75it/s, loss=3.466]\n","output_type":"stream"},{"name":"stdout","text":"Example 1 CORRECT: Dog on a beach by the ocean\nExample 1 OUTPUT: <SOS> a young boy is jumping into the air on a beach . <EOS>\nExample 2 CORRECT: Child holding red frisbee outdoors\nExample 2 OUTPUT: <SOS> a man in a red shirt is standing on a rock in a field . <EOS>\nExample 3 CORRECT: Bus driving by parked cars\nExample 3 OUTPUT: <SOS> a man in a blue shirt is standing on a rock with a rope in the background . <EOS>\nExample 4 CORRECT: A small boat in the ocean\nExample 4 OUTPUT: <SOS> a man is climbing a rock wall . <EOS>\nExample 5 CORRECT: A cowboy riding a horse in the desert\nExample 5 OUTPUT: <SOS> a man is climbing a rock wall . <EOS>\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 05: 100%|██████████| 2528/2528 [24:10<00:00,  1.74it/s, loss=3.681]\n","output_type":"stream"},{"name":"stdout","text":"Example 1 CORRECT: Dog on a beach by the ocean\nExample 1 OUTPUT: <SOS> a young girl in a blue bathing suit is jumping into the water . <EOS>\nExample 2 CORRECT: Child holding red frisbee outdoors\nExample 2 OUTPUT: <SOS> a football player in a red jersey is running with the ball . <EOS>\nExample 3 CORRECT: Bus driving by parked cars\nExample 3 OUTPUT: <SOS> a man is riding a bicycle on a dirt road . <EOS>\nExample 4 CORRECT: A small boat in the ocean\nExample 4 OUTPUT: <SOS> a man is surfing on a wave . <EOS>\nExample 5 CORRECT: A cowboy riding a horse in the desert\nExample 5 OUTPUT: <SOS> a man is standing on a beach with a dog . <EOS>\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 06: 100%|██████████| 2528/2528 [24:09<00:00,  1.74it/s, loss=3.484]\n","output_type":"stream"},{"name":"stdout","text":"Example 1 CORRECT: Dog on a beach by the ocean\nExample 1 OUTPUT: <SOS> a boy in a blue shirt is jumping into the ocean . <EOS>\nExample 2 CORRECT: Child holding red frisbee outdoors\nExample 2 OUTPUT: <SOS> a man in a red shirt and white shorts is jumping on the grass . <EOS>\nExample 3 CORRECT: Bus driving by parked cars\nExample 3 OUTPUT: <SOS> a man in a yellow shirt and a woman in a black jacket is standing on a rock overlooking a waterfall . <EOS>\nExample 4 CORRECT: A small boat in the ocean\nExample 4 OUTPUT: <SOS> a man in a wetsuit is surfing . <EOS>\nExample 5 CORRECT: A cowboy riding a horse in the desert\nExample 5 OUTPUT: <SOS> a man in a blue shirt and a woman in a black shirt is walking on the beach . <EOS>\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 07: 100%|██████████| 2528/2528 [24:08<00:00,  1.75it/s, loss=3.497]\n","output_type":"stream"},{"name":"stdout","text":"Example 1 CORRECT: Dog on a beach by the ocean\nExample 1 OUTPUT: <SOS> a boy is jumping into the air on the sand . <EOS>\nExample 2 CORRECT: Child holding red frisbee outdoors\nExample 2 OUTPUT: <SOS> a man in a red shirt and black shorts is jumping a ramp . <EOS>\nExample 3 CORRECT: Bus driving by parked cars\nExample 3 OUTPUT: <SOS> a man in a blue shirt and a woman in a green jacket sit on a bench . <EOS>\nExample 4 CORRECT: A small boat in the ocean\nExample 4 OUTPUT: <SOS> a black dog running through the water . <EOS>\nExample 5 CORRECT: A cowboy riding a horse in the desert\nExample 5 OUTPUT: <SOS> a man is jumping a ramp on his skateboard . <EOS>\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 08: 100%|██████████| 2528/2528 [24:07<00:00,  1.75it/s, loss=3.480]\n","output_type":"stream"},{"name":"stdout","text":"Example 1 CORRECT: Dog on a beach by the ocean\nExample 1 OUTPUT: <SOS> a dog is jumping into the water . <EOS>\nExample 2 CORRECT: Child holding red frisbee outdoors\nExample 2 OUTPUT: <SOS> a man in a red shirt is climbing a rock . <EOS>\nExample 3 CORRECT: Bus driving by parked cars\nExample 3 OUTPUT: <SOS> a man is standing on a rocky mountain . <EOS>\nExample 4 CORRECT: A small boat in the ocean\nExample 4 OUTPUT: <SOS> a man is standing on top of a mountain . <EOS>\nExample 5 CORRECT: A cowboy riding a horse in the desert\nExample 5 OUTPUT: <SOS> a man is standing on top of a mountain . <EOS>\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 09: 100%|██████████| 2528/2528 [24:13<00:00,  1.74it/s, loss=3.287]\n","output_type":"stream"},{"name":"stdout","text":"Example 1 CORRECT: Dog on a beach by the ocean\nExample 1 OUTPUT: <SOS> a young boy is jumping into the sand at the beach . <EOS>\nExample 2 CORRECT: Child holding red frisbee outdoors\nExample 2 OUTPUT: <SOS> a man in a red shirt is riding a dirt bike through a forest . <EOS>\nExample 3 CORRECT: Bus driving by parked cars\nExample 3 OUTPUT: <SOS> a man in a yellow shirt is standing on a rock overlooking a waterfall . <EOS>\nExample 4 CORRECT: A small boat in the ocean\nExample 4 OUTPUT: <SOS> a man in a black wetsuit is riding a wave . <EOS>\nExample 5 CORRECT: A cowboy riding a horse in the desert\nExample 5 OUTPUT: <SOS> a man in a blue shirt is standing on a rock in front of a building . <EOS>\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 10: 100%|██████████| 2528/2528 [24:13<00:00,  1.74it/s, loss=3.514]\n","output_type":"stream"},{"name":"stdout","text":"Example 1 CORRECT: Dog on a beach by the ocean\nExample 1 OUTPUT: <SOS> a young boy is jumping in the sand on the beach . <EOS>\nExample 2 CORRECT: Child holding red frisbee outdoors\nExample 2 OUTPUT: <SOS> a man in a red shirt and white shorts is riding a bike on a dirt road . <EOS>\nExample 3 CORRECT: Bus driving by parked cars\nExample 3 OUTPUT: <SOS> a man is standing on a hill looking at a woman . <EOS>\nExample 4 CORRECT: A small boat in the ocean\nExample 4 OUTPUT: <SOS> a man is standing on a cliff overlooking the ocean . <EOS>\nExample 5 CORRECT: A cowboy riding a horse in the desert\nExample 5 OUTPUT: <SOS> a man is standing on the beach with a dog . <EOS>\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 11: 100%|██████████| 2528/2528 [24:12<00:00,  1.74it/s, loss=2.976]\n","output_type":"stream"},{"name":"stdout","text":"Example 1 CORRECT: Dog on a beach by the ocean\nExample 1 OUTPUT: <SOS> a girl in a blue shirt is running on the beach . <EOS>\nExample 2 CORRECT: Child holding red frisbee outdoors\nExample 2 OUTPUT: <SOS> a man in a red shirt and black pants is climbing a rock face . <EOS>\nExample 3 CORRECT: Bus driving by parked cars\nExample 3 OUTPUT: <SOS> a man and a woman are walking down a city street . <EOS>\nExample 4 CORRECT: A small boat in the ocean\nExample 4 OUTPUT: <SOS> a man is surfing on a wave . <EOS>\nExample 5 CORRECT: A cowboy riding a horse in the desert\nExample 5 OUTPUT: <SOS> a man in a blue shirt is walking along a mountain path . <EOS>\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 12: 100%|██████████| 2528/2528 [24:12<00:00,  1.74it/s, loss=2.963]\n","output_type":"stream"},{"name":"stdout","text":"Example 1 CORRECT: Dog on a beach by the ocean\nExample 1 OUTPUT: <SOS> a girl in a blue bathing suit is jumping into the air on the sand . <EOS>\nExample 2 CORRECT: Child holding red frisbee outdoors\nExample 2 OUTPUT: <SOS> a man in a red shirt and helmet is climbing a rock face . <EOS>\nExample 3 CORRECT: Bus driving by parked cars\nExample 3 OUTPUT: <SOS> a man in a yellow jacket and black pants is sitting on a rock in front of a waterfall . <EOS>\nExample 4 CORRECT: A small boat in the ocean\nExample 4 OUTPUT: <SOS> a man is standing on a cliff with a fishing pole in the background . <EOS>\nExample 5 CORRECT: A cowboy riding a horse in the desert\nExample 5 OUTPUT: <SOS> a man in a black jacket and hat is standing on a rocky ledge overlooking the ocean . <EOS>\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 13: 100%|██████████| 2528/2528 [24:14<00:00,  1.74it/s, loss=3.182]\n","output_type":"stream"},{"name":"stdout","text":"Example 1 CORRECT: Dog on a beach by the ocean\nExample 1 OUTPUT: <SOS> a girl in a blue bathing suit is jumping into the sand at the beach . <EOS>\nExample 2 CORRECT: Child holding red frisbee outdoors\nExample 2 OUTPUT: <SOS> a little girl in a red shirt and blue shorts is swinging on a rope swing . <EOS>\nExample 3 CORRECT: Bus driving by parked cars\nExample 3 OUTPUT: <SOS> a boy on a skateboard is airborne . <EOS>\nExample 4 CORRECT: A small boat in the ocean\nExample 4 OUTPUT: <SOS> a dog is swimming in the water . <EOS>\nExample 5 CORRECT: A cowboy riding a horse in the desert\nExample 5 OUTPUT: <SOS> a man in a black shirt is climbing a mountain . <EOS>\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 14: 100%|██████████| 2528/2528 [24:14<00:00,  1.74it/s, loss=3.119]\n","output_type":"stream"},{"name":"stdout","text":"Example 1 CORRECT: Dog on a beach by the ocean\nExample 1 OUTPUT: <SOS> a girl is jumping in the sand at the beach . <EOS>\nExample 2 CORRECT: Child holding red frisbee outdoors\nExample 2 OUTPUT: <SOS> a boy in a red shirt is riding a bicycle on a dirt track . <EOS>\nExample 3 CORRECT: Bus driving by parked cars\nExample 3 OUTPUT: <SOS> a skateboarder in the air above a ramp . <EOS>\nExample 4 CORRECT: A small boat in the ocean\nExample 4 OUTPUT: <SOS> a man is standing on top of a mountain with his arms out to the right . <EOS>\nExample 5 CORRECT: A cowboy riding a horse in the desert\nExample 5 OUTPUT: <SOS> a group of people are standing on the sand at the beach . <EOS>\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 15: 100%|██████████| 2528/2528 [24:10<00:00,  1.74it/s, loss=3.074]\n","output_type":"stream"},{"name":"stdout","text":"Example 1 CORRECT: Dog on a beach by the ocean\nExample 1 OUTPUT: <SOS> a girl in a white shirt is running along a beach . <EOS>\nExample 2 CORRECT: Child holding red frisbee outdoors\nExample 2 OUTPUT: <SOS> a man in a grey shirt and a white hat is riding a bike . <EOS>\nExample 3 CORRECT: Bus driving by parked cars\nExample 3 OUTPUT: <SOS> a man in a yellow shirt and blue jeans is standing on a rock overlooking a waterfall . <EOS>\nExample 4 CORRECT: A small boat in the ocean\nExample 4 OUTPUT: <SOS> a dog is running through the snow . <EOS>\nExample 5 CORRECT: A cowboy riding a horse in the desert\nExample 5 OUTPUT: <SOS> a man in a white shirt and jeans is riding a bicycle on a <UNK> street . <EOS>\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 16: 100%|██████████| 2528/2528 [24:05<00:00,  1.75it/s, loss=3.076]\n","output_type":"stream"},{"name":"stdout","text":"Example 1 CORRECT: Dog on a beach by the ocean\nExample 1 OUTPUT: <SOS> a little girl is running on the beach . <EOS>\nExample 2 CORRECT: Child holding red frisbee outdoors\nExample 2 OUTPUT: <SOS> a dirt biker rides through the dirt . <EOS>\nExample 3 CORRECT: Bus driving by parked cars\nExample 3 OUTPUT: <SOS> a man in a yellow jacket stands on a rock in the mountains . <EOS>\nExample 4 CORRECT: A small boat in the ocean\nExample 4 OUTPUT: <SOS> a brown dog is running through the water . <EOS>\nExample 5 CORRECT: A cowboy riding a horse in the desert\nExample 5 OUTPUT: <SOS> a man in a blue shirt and shorts is standing on a rock in front of a mountain . <EOS>\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 17: 100%|██████████| 2528/2528 [24:05<00:00,  1.75it/s, loss=2.971]\n","output_type":"stream"},{"name":"stdout","text":"Example 1 CORRECT: Dog on a beach by the ocean\nExample 1 OUTPUT: <SOS> a woman in a white shirt and a woman in a blue bikini running on the beach . <EOS>\nExample 2 CORRECT: Child holding red frisbee outdoors\nExample 2 OUTPUT: <SOS> a person in a red helmet and helmet is riding a dirt bike on a dirt path . <EOS>\nExample 3 CORRECT: Bus driving by parked cars\nExample 3 OUTPUT: <SOS> a skateboarder does a trick on a ramp . <EOS>\nExample 4 CORRECT: A small boat in the ocean\nExample 4 OUTPUT: <SOS> a black dog is swimming in water . <EOS>\nExample 5 CORRECT: A cowboy riding a horse in the desert\nExample 5 OUTPUT: <SOS> a man in a white shirt and jeans walks along a mountain . <EOS>\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 18: 100%|██████████| 2528/2528 [24:11<00:00,  1.74it/s, loss=3.188]\n","output_type":"stream"},{"name":"stdout","text":"Example 1 CORRECT: Dog on a beach by the ocean\nExample 1 OUTPUT: <SOS> a young boy is running along a beach . <EOS>\nExample 2 CORRECT: Child holding red frisbee outdoors\nExample 2 OUTPUT: <SOS> a person in a red and white uniform is riding a dirt bike . <EOS>\nExample 3 CORRECT: Bus driving by parked cars\nExample 3 OUTPUT: <SOS> a boy is doing a trick on his skateboard in midair . <EOS>\nExample 4 CORRECT: A small boat in the ocean\nExample 4 OUTPUT: <SOS> a dog is swimming in the water . <EOS>\nExample 5 CORRECT: A cowboy riding a horse in the desert\nExample 5 OUTPUT: <SOS> a man in a blue shirt is riding a brown horse . <EOS>\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 19: 100%|██████████| 2528/2528 [24:13<00:00,  1.74it/s, loss=2.645]\n","output_type":"stream"},{"name":"stdout","text":"Example 1 CORRECT: Dog on a beach by the ocean\nExample 1 OUTPUT: <SOS> two women in bathing suits play in the sand . <EOS>\nExample 2 CORRECT: Child holding red frisbee outdoors\nExample 2 OUTPUT: <SOS> a group of children are playing with a red and white dog . <EOS>\nExample 3 CORRECT: Bus driving by parked cars\nExample 3 OUTPUT: <SOS> a skateboarder is doing a trick in the air . <EOS>\nExample 4 CORRECT: A small boat in the ocean\nExample 4 OUTPUT: <SOS> a dog swimming in the water . <EOS>\nExample 5 CORRECT: A cowboy riding a horse in the desert\nExample 5 OUTPUT: <SOS> two women in bikinis are walking on the sand . <EOS>\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 20: 100%|██████████| 2528/2528 [24:13<00:00,  1.74it/s, loss=2.539]\n","output_type":"stream"},{"name":"stdout","text":"Example 1 CORRECT: Dog on a beach by the ocean\nExample 1 OUTPUT: <SOS> a young girl is running on the beach . <EOS>\nExample 2 CORRECT: Child holding red frisbee outdoors\nExample 2 OUTPUT: <SOS> a person in a red and white suit is riding a bike on a dirt path . <EOS>\nExample 3 CORRECT: Bus driving by parked cars\nExample 3 OUTPUT: <SOS> two people are sitting on a rock overlooking a river . <EOS>\nExample 4 CORRECT: A small boat in the ocean\nExample 4 OUTPUT: <SOS> a dog swimming in the water . <EOS>\nExample 5 CORRECT: A cowboy riding a horse in the desert\nExample 5 OUTPUT: <SOS> a man in a blue shirt is riding a brown horse on a beach . <EOS>\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 21: 100%|██████████| 2528/2528 [24:14<00:00,  1.74it/s, loss=2.804]\n","output_type":"stream"},{"name":"stdout","text":"Example 1 CORRECT: Dog on a beach by the ocean\nExample 1 OUTPUT: <SOS> two women play volleyball on the beach . <EOS>\nExample 2 CORRECT: Child holding red frisbee outdoors\nExample 2 OUTPUT: <SOS> a person in a red helmet riding a dirt bike on a dirt path . <EOS>\nExample 3 CORRECT: Bus driving by parked cars\nExample 3 OUTPUT: <SOS> a young boy skateboards at a skate park . <EOS>\nExample 4 CORRECT: A small boat in the ocean\nExample 4 OUTPUT: <SOS> a dog is jumping up at the man wearing a blue shirt . <EOS>\nExample 5 CORRECT: A cowboy riding a horse in the desert\nExample 5 OUTPUT: <SOS> a black dog is running across a grassy field . <EOS>\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 22: 100%|██████████| 2528/2528 [24:13<00:00,  1.74it/s, loss=2.731]\n","output_type":"stream"},{"name":"stdout","text":"Example 1 CORRECT: Dog on a beach by the ocean\nExample 1 OUTPUT: <SOS> a young boy is running along a sandy beach . <EOS>\nExample 2 CORRECT: Child holding red frisbee outdoors\nExample 2 OUTPUT: <SOS> a person in a red shirt and helmet is riding a dirt bike . <EOS>\nExample 3 CORRECT: Bus driving by parked cars\nExample 3 OUTPUT: <SOS> a boy skateboards off of a cement ramp . <EOS>\nExample 4 CORRECT: A small boat in the ocean\nExample 4 OUTPUT: <SOS> a dog is swimming in the water . <EOS>\nExample 5 CORRECT: A cowboy riding a horse in the desert\nExample 5 OUTPUT: <SOS> a man in a blue shirt is standing in the sand on a pink snowboard . <EOS>\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 23: 100%|██████████| 2528/2528 [24:13<00:00,  1.74it/s, loss=2.638]\n","output_type":"stream"},{"name":"stdout","text":"Example 1 CORRECT: Dog on a beach by the ocean\nExample 1 OUTPUT: <SOS> a young girl is running on the beach . <EOS>\nExample 2 CORRECT: Child holding red frisbee outdoors\nExample 2 OUTPUT: <SOS> a person in a red jacket and white helmet is riding a dirt bike on a path . <EOS>\nExample 3 CORRECT: Bus driving by parked cars\nExample 3 OUTPUT: <SOS> two people are walking across a street that is snow covered . <EOS>\nExample 4 CORRECT: A small boat in the ocean\nExample 4 OUTPUT: <SOS> a dog swims through water . <EOS>\nExample 5 CORRECT: A cowboy riding a horse in the desert\nExample 5 OUTPUT: <SOS> a man in a grey shirt is standing in front of a mountain range . <EOS>\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 24: 100%|██████████| 2528/2528 [24:10<00:00,  1.74it/s, loss=2.617]\n","output_type":"stream"},{"name":"stdout","text":"Example 1 CORRECT: Dog on a beach by the ocean\nExample 1 OUTPUT: <SOS> two people are standing on the beach . <EOS>\nExample 2 CORRECT: Child holding red frisbee outdoors\nExample 2 OUTPUT: <SOS> a man in a red helmet riding a dirt bike . <EOS>\nExample 3 CORRECT: Bus driving by parked cars\nExample 3 OUTPUT: <SOS> two people are sitting in a bus stop . <EOS>\nExample 4 CORRECT: A small boat in the ocean\nExample 4 OUTPUT: <SOS> a brown and white dog is jumping in the water . <EOS>\nExample 5 CORRECT: A cowboy riding a horse in the desert\nExample 5 OUTPUT: <SOS> a man and a woman walk their dogs . <EOS>\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 25: 100%|██████████| 2528/2528 [24:05<00:00,  1.75it/s, loss=2.366]\n","output_type":"stream"},{"name":"stdout","text":"Example 1 CORRECT: Dog on a beach by the ocean\nExample 1 OUTPUT: <SOS> a group of people are standing on a beach . <EOS>\nExample 2 CORRECT: Child holding red frisbee outdoors\nExample 2 OUTPUT: <SOS> a man on a bike is riding on a dirt bike . <EOS>\nExample 3 CORRECT: Bus driving by parked cars\nExample 3 OUTPUT: <SOS> two people sit in a pool on a hillside . <EOS>\nExample 4 CORRECT: A small boat in the ocean\nExample 4 OUTPUT: <SOS> a dog is swimming in a lake with a stick in his mouth . <EOS>\nExample 5 CORRECT: A cowboy riding a horse in the desert\nExample 5 OUTPUT: <SOS> a dog is running through the sand . <EOS>\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 26: 100%|██████████| 2528/2528 [24:05<00:00,  1.75it/s, loss=2.583]\n","output_type":"stream"},{"name":"stdout","text":"Example 1 CORRECT: Dog on a beach by the ocean\nExample 1 OUTPUT: <SOS> two women play beach volleyball on the beach . <EOS>\nExample 2 CORRECT: Child holding red frisbee outdoors\nExample 2 OUTPUT: <SOS> a person in a red shirt is riding a dirt bike over dirt . <EOS>\nExample 3 CORRECT: Bus driving by parked cars\nExample 3 OUTPUT: <SOS> a boy does tricks on his skateboard . <EOS>\nExample 4 CORRECT: A small boat in the ocean\nExample 4 OUTPUT: <SOS> a person jumping off of a dock into a lake . <EOS>\nExample 5 CORRECT: A cowboy riding a horse in the desert\nExample 5 OUTPUT: <SOS> a man in a blue shirt and dark shorts is doing a back flip on a beach . <EOS>\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 27: 100%|██████████| 2528/2528 [24:09<00:00,  1.74it/s, loss=2.168]\n","output_type":"stream"},{"name":"stdout","text":"Example 1 CORRECT: Dog on a beach by the ocean\nExample 1 OUTPUT: <SOS> a boy is leaping into the air whilst playing in the sand . <EOS>\nExample 2 CORRECT: Child holding red frisbee outdoors\nExample 2 OUTPUT: <SOS> a person in a red and white suit rides a bike down a hill in the woods . <EOS>\nExample 3 CORRECT: Bus driving by parked cars\nExample 3 OUTPUT: <SOS> a boy does a skateboard trick off of a graffiti covered wall . <EOS>\nExample 4 CORRECT: A small boat in the ocean\nExample 4 OUTPUT: <SOS> a dog is jumping through the water . <EOS>\nExample 5 CORRECT: A cowboy riding a horse in the desert\nExample 5 OUTPUT: <SOS> a man in a brown shirt holding a camera on a mountain . <EOS>\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 28: 100%|██████████| 2528/2528 [24:12<00:00,  1.74it/s, loss=2.247]\n","output_type":"stream"},{"name":"stdout","text":"Example 1 CORRECT: Dog on a beach by the ocean\nExample 1 OUTPUT: <SOS> two women play volleyball on the beach . <EOS>\nExample 2 CORRECT: Child holding red frisbee outdoors\nExample 2 OUTPUT: <SOS> a man in a red shirt and helmet riding a bike down a dirt path . <EOS>\nExample 3 CORRECT: Bus driving by parked cars\nExample 3 OUTPUT: <SOS> a boy does a skateboard trick while a crowd watches . <EOS>\nExample 4 CORRECT: A small boat in the ocean\nExample 4 OUTPUT: <SOS> a large black and brown dog is swimming in a deep water . <EOS>\nExample 5 CORRECT: A cowboy riding a horse in the desert\nExample 5 OUTPUT: <SOS> a man in a blue shirt is riding a skateboard up a ramp . <EOS>\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 29:  66%|██████▌   | 1663/2528 [15:56<08:17,  1.74it/s, loss=2.310]","output_type":"stream"}],"execution_count":null}]}